{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wata_ruja/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn import cross_validation, preprocessing, linear_model #機械学習用のライブラリを利用\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#警告文を無視\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "sklearn.__version__\n",
    "\n",
    "#データのimport\n",
    "train_preprocessed = pd.read_csv('train_preprocessed.csv', index_col=0)\n",
    "y_train = train_preprocessed['y'].values\n",
    "X_temp = train_preprocessed.drop('y', axis= 1 )\n",
    "X_train = X_temp.values\n",
    "X_temp.columns\n",
    "\n",
    "#評価関数の定義\n",
    "def rmsle(predicted, real):\n",
    "    return np.sqrt(np.mean((np.log(real+1) - np.log(predicted+1))**2))\n",
    "\n",
    "rmsle_score = make_scorer(rmsle, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridgeでの偏回帰係数\n",
      "9.685856312076814\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "{'alpha': 620} 0.26694527128489454\n",
      "cv_score: 5179.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#5：Ridge Regressorを適用する------------------------------------------- -> Ridge回帰を採択\n",
    "ridge_params = {\n",
    "    'alpha' : [10,540,579,620,626,630,800,1000]\n",
    "} \n",
    "# for temp in alpha_list:\n",
    "clf_ridge= linear_model.Ridge(alpha=0.7)#0.7\n",
    "clf_ridge.fit(X_train, y_train)\n",
    "print(\"\\nRidgeでの偏回帰係数\")\n",
    "print(clf_ridge.intercept_) \n",
    "# print(np.sort(abs(clf_ridge_J1.coef_)))\n",
    "# print(clf_ridge_J1.coef_) \n",
    "\n",
    "\n",
    "clf_ridge =  linear_model.Ridge()#0.7\n",
    "# # ハイパーパラメータ探索\n",
    "clf_ridge_cv = GridSearchCV(clf_ridge, ridge_params, scoring= rmsle_score, cv=5,verbose=1)\n",
    "clf_ridge_cv.fit(X_train, np.exp(y_train))\n",
    "print(clf_ridge_cv.best_params_, -1*clf_ridge_cv.best_score_)\n",
    "clf_ridge = linear_model.Ridge( **clf_ridge_cv.best_params_)\n",
    "clf_ridge.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "scores = -cross_validation.cross_val_score(clf_ridge, X_train, np.exp(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "print( \"cv_score: %0.3f\" % (scores.mean()**(1/2)))\n",
    "\n",
    "#モデルの保存\n",
    "with open('model_ridge.pickle', mode='wb') as fp:\n",
    "    pickle.dump(clf_ridge, fp, protocol=2)\n",
    "\n",
    "df_importance_ridge = pd.DataFrame(clf_ridge.coef_,X_temp.columns).reset_index()\n",
    "df_importance_ridge = df_importance_ridge.rename(columns={'index': 'val_name', 0:'importance'})\n",
    "df_importance_ridge = df_importance_ridge.sort_values('importance', ascending=False)\n",
    "df_importance_ridge['ridge_importance_per_score'] = df_importance_ridge['importance']/ scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridgeでの偏回帰係数\n",
      "9.68562198861292\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha_1': 10, 'alpha_2': 15, 'lambda_1': 2.0, 'lambda_2': 15} 0.2792136742637733\n",
      "cv_score: 5148.393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "#5：Ridge Regressorを適用する------------------------------------------- -> Ridge回帰を採択\n",
    "Bridge_params = {\n",
    "    'alpha_1' : [10,13],\n",
    "    'alpha_2' : [11,12,15],\n",
    "    'lambda_1' : [2.0,3.0],\n",
    "    'lambda_2' : [15,20],\n",
    "} \n",
    "# for temp in alpha_list:\n",
    "clf_Bridge= BayesianRidge()#0.7\n",
    "clf_Bridge.fit(X_train, y_train)\n",
    "print(\"\\nRidgeでの偏回帰係数\")\n",
    "print(clf_Bridge.intercept_) \n",
    "# print(np.sort(abs(clf_ridge_J1.coef_)))\n",
    "# print(clf_ridge_J1.coef_) \n",
    "\n",
    "\n",
    "clf_Bridge =  BayesianRidge()#0.7\n",
    "# # ハイパーパラメータ探索\n",
    "clf_Bridge_cv = GridSearchCV(clf_Bridge, Bridge_params, scoring= rmsle_score, cv=5,verbose=1)\n",
    "clf_Bridge_cv.fit(X_train, np.exp(y_train))\n",
    "print(clf_Bridge_cv.best_params_, -1*clf_Bridge_cv.best_score_)\n",
    "clf_Bridge = BayesianRidge( **clf_Bridge_cv.best_params_)\n",
    "clf_Bridge.fit(X_train, y_train)\n",
    "\n",
    "#　交差検証\n",
    "scores = -cross_validation.cross_val_score(clf_Bridge, X_train, np.exp(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "print( \"cv_score: %0.3f\" % (scores.mean()**(1/2)))\n",
    "\n",
    "#モデルの保存\n",
    "with open('model_Bridge.pickle', mode='wb') as fp:\n",
    "    pickle.dump(clf_Bridge, fp, protocol=2)\n",
    "\n",
    "    \n",
    "df_importance_Bridge = pd.DataFrame(clf_Bridge.coef_,X_temp.columns).reset_index()\n",
    "df_importance_Bridge = df_importance_Bridge.rename(columns={'index': 'val_name', 0:'importance'})\n",
    "df_importance_Bridge = df_importance_Bridge.sort_values('importance', ascending=False)\n",
    "df_importance_Bridge['bridge_importance_per_score'] = df_importance_Bridge['importance']/ scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.1, 'learning_rate': 0.09, 'max_depth': 6, 'min_child_weight': 3, 'reg_alpha': 15, 'reg_lambda': 15, 'seed': 71, 'subsample': 0.8} 0.24976381338690254\n",
      "cv_score: 4738.677\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "xgb_params = {\n",
    "#      'n_estimators': [1000],\n",
    "      'learning_rate':[0.09],\n",
    "     #max_features': 0.2,\n",
    "    'gamma':[0.1],\n",
    "    'max_depth': [6],\n",
    "    'min_child_weight':[ 3],\n",
    "     'reg_alpha':[10,15],\n",
    "      'reg_lambda':[15],\n",
    "    'seed':[71],\n",
    "    'subsample':[0.8],\n",
    "#     'min_samples_leaf': [2],\n",
    "#     'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "clf= xgb.XGBRegressor()#0.7\n",
    "# ハイパーパラメータ探索\n",
    "clf_xgb_cv = GridSearchCV(clf, xgb_params, scoring= rmsle_score, cv=5,verbose=1)\n",
    "clf_xgb_cv.fit(X_train, np.exp(y_train))\n",
    "print(clf_xgb_cv.best_params_, -1*clf_xgb_cv.best_score_)\n",
    "\n",
    "clf= xgb.XGBRegressor(**clf_xgb_cv.best_params_)#0.7\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "scores = -cross_validation.cross_val_score(clf, X_train, np.exp(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "print( \"cv_score: %0.3f\" % (scores.mean()**(1/2)))\n",
    "\n",
    "with open('model_xgb.pickle', mode='wb') as fp:\n",
    "    pickle.dump(clf, fp,protocol=2)\n",
    "    \n",
    "df_importance_xgb = pd.DataFrame(clf.feature_importances_ ,X_temp.columns).reset_index()\n",
    "df_importance_xgb = df_importance_xgb.rename(columns={'index': 'val_name', 0:'importance'})\n",
    "df_importance_xgb = df_importance_xgb.sort_values('importance', ascending=False)\n",
    "df_importance_xgb['xgb_importance_per_score'] = df_importance_xgb['importance']/ scores.mean()\n",
    "# df_importance_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   27.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 14, 'min_samples_leaf': 2, 'n_estimators': 1000, 'n_jobs': -1, 'random_state': 0} 0.26376688050103664\n",
      "cv_score: 4871.034\n"
     ]
    }
   ],
   "source": [
    "#ランダムフォレスト\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_params ={\n",
    "    'random_state':[0],\n",
    "     'n_estimators':[1000],\n",
    "    'min_samples_leaf':[2], \n",
    "     'max_depth':[14],\n",
    "#             criterion=/rmsle_score,\n",
    "            'n_jobs':[-1],                        \n",
    "}\n",
    "clf_rf =RandomForestRegressor()\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # ハイパーパラメータ探索\n",
    "clf_rf_cv = GridSearchCV(clf_rf, rf_params, scoring= rmsle_score, cv=5,verbose=1)\n",
    "clf_rf_cv.fit(X_train, np.exp(y_train))\n",
    "print(clf_rf_cv.best_params_, -1*clf_rf_cv.best_score_)\n",
    "\n",
    "clf_rf= RandomForestRegressor(**clf_rf_cv.best_params_)#0.7\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "scores = -cross_validation.cross_val_score(clf_rf, X_train, np.exp(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "print( \"cv_score: %0.3f\" % (scores.mean()**(1/2)))\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "with open('model_rf.pickle', mode='wb') as fp:\n",
    "    pickle.dump(clf_rf, fp, protocol=2)\n",
    "\n",
    "    \n",
    "df_importance_rf= pd.DataFrame(clf_rf.feature_importances_ ,X_temp.columns).reset_index()\n",
    "df_importance_rf = df_importance_rf.rename(columns={'index': 'val_name', 0:'importance'})\n",
    "df_importance_rf = df_importance_rf.sort_values('importance', ascending=False)\n",
    "df_importance_rf['rf_importance_per_score'] = df_importance_rf['importance']/ scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_score: 4657.932\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "# 上記のパラメータでモデルを学習する\n",
    "clf_lgb = lgb.LGBMRegressor(lerning_rate=0.09, max_depth=5, n_estimators=100,min_child_samples=5, reg_lambda=5, reg_alpha=5)\n",
    "clf_lgb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # # ハイパーパラメータ探索\n",
    "# clf_cv = GridSearchCV(clf_rf, rf_params, scoring= rmsle_score, cv=5,verbose=1)\n",
    "# clf_cv.fit(X_train, np.exp(y_train))\n",
    "# print(clf_cv.best_params_, -1*clf_cv.best_score_)\n",
    "\n",
    "# # clf_rf= RandomForestRegressor(**clf_cv.best_params_)#0.7\n",
    "# clf_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "scores = -cross_validation.cross_val_score(clf_lgb, X_train, np.exp(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "print( \"cv_score: %0.3f\" % (scores.mean()**(1/2)))\n",
    "\n",
    "clf_lgb.fit(X_train, y_train)\n",
    "with open('model_lgb.pickle', mode='wb') as fp:\n",
    "    pickle.dump(clf_lgb, fp, protocol=2)\n",
    "\n",
    "    \n",
    "df_importance_lgb= pd.DataFrame(clf_lgb.feature_importances_ ,X_temp.columns).reset_index()\n",
    "df_importance_lgb = df_importance_lgb.rename(columns={'index': 'val_name', 0:'importance'})\n",
    "df_importance_lgb = df_importance_lgb.sort_values('importance', ascending=False)\n",
    "df_importance_lgb['lgb_importance_per_score'] = df_importance_lgb['importance']/ scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "est_params = {\n",
    "    'n_estimators':[100],\n",
    "    'learning_rate':[0.1],\n",
    "    'max_depth':[4,5,6],\n",
    "    'random_state':[0]\n",
    "}\n",
    "clf_est = GradientBoostingRegressor()\n",
    "\n",
    "# # # ハイパーパラメータ探索\n",
    "clf_cv = GridSearchCV(clf_est, est_params, scoring= rmsle_score, cv=5,verbose=1)\n",
    "clf_cv.fit(X_train, np.exp(y_train))\n",
    "print(clf_cv.best_params_, -1*clf_cv.best_score_)\n",
    "\n",
    "\n",
    "clf_est= GradientBoostingRegressor(**clf_cv.best_params_)#0.7\n",
    "clf_est.fit(X_train, y_train)\n",
    "\n",
    "scores = -cross_validation.cross_val_score(clf_est, X_train, np.exp(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "print( \"cv_score: %0.3f\" % (scores.mean() **(1/2)))\n",
    "\n",
    "clf_est.fit(X_train, y_train)\n",
    "with open('model_est.pickle', mode='wb') as fp:\n",
    "    pickle.dump(clf_est, fp, protocol=2)\n",
    "\n",
    "    \n",
    "df_importance_est= pd.DataFrame(clf_est.feature_importances_ ,X_temp.columns).reset_index()\n",
    "df_importance_est = df_importance_est.rename(columns={'index': 'val_name', 0:'importance'})\n",
    "df_importance_est = df_importance_est.sort_values('importance', ascending=False)\n",
    "df_importance_est['est_importance_per_score'] = df_importance_est['importance']/ scores.mean()\n",
    "\n",
    "# df_importance_est['est_importance_per_score'] = df_importance_est['importance']/ scores.mean()\n",
    "# df_importance_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 標準偏回帰係数を並べてみた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = df_importance_ridge.sort_index()[['val_name', 'ridge_importance_per_score']]\n",
    "df_importance['bridge_importance_per_score'] = df_importance_Bridge.sort_index()['bridge_importance_per_score']\n",
    "df_importance['xgb_importance_per_score'] = df_importance_xgb.sort_index()['xgb_importance_per_score']\n",
    "df_importance['rf_importance_per_score'] = df_importance_rf.sort_index()['rf_importance_per_score']\n",
    "df_importance['lgb_importance_per_score'] = df_importance_lgb.sort_index()['lgb_importance_per_score']\n",
    "df_importance['est_importance_per_score'] = df_importance_est.sort_index()['est_importance_per_score']\n",
    "\n",
    "import scipy.stats\n",
    "def SS(text):\n",
    "    return scipy.stats.zscore(text)\n",
    "\n",
    "df_importance['ridge_importance_per_score'] = SS(df_importance['ridge_importance_per_score'])\n",
    "df_importance['xgb_importance_per_score'] =SS(df_importance['xgb_importance_per_score'])\n",
    "df_importance['rf_importance_per_score'] = SS(df_importance['rf_importance_per_score'])\n",
    "df_importance['lgb_importance_per_score'] =SS(df_importance['lgb_importance_per_score'])\n",
    "df_importance['est_importance_per_score'] =SS(df_importance['est_importance_per_score'])\n",
    "\n",
    "df_importance['total_importance'] =  (df_importance['xgb_importance_per_score'] +  df_importance['rf_importance_per_score'] +  df_importance['ridge_importance_per_score'] +  df_importance['lgb_importance_per_score'] +df_importance['est_importance_per_score']  ) /4\n",
    "df_importance= df_importance.sort_values('total_importance', ascending=False)\n",
    "df_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # アンサンブル学習 スタッキングを実装　最終出力は３つのモデルの調和平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackingClassifer():\n",
    "    def __init__(self, estimators, estimators_second, merge_estimator, ):\n",
    "        \n",
    "        self.original_clfs = dict(estimators)\n",
    "        self.original_clfs_second = dict(estimators_second)\n",
    "\n",
    "        self.m_clf = merge_estimator\n",
    "        \n",
    "        self.clf_dict = defaultdict(list)\n",
    "        self.clf_dict_second = defaultdict(list)\n",
    "        self.clfs_index = sorted(self.original_clfs.keys())\n",
    "        self.clfs_index_second = sorted(self.original_clfs_second.keys())\n",
    "\n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        #リストの初期化\n",
    "        self.clfs_dict =  defaultdict(list) \n",
    "        #交差検証(k=5)\n",
    "        K = 5\n",
    "        kf = KFold(n_splits=K, shuffle=True, random_state=1 )\n",
    "\n",
    "        #indexのリストを作成\n",
    "        index_list = list(kf.split(X, y))\n",
    "        \n",
    "        #第一段の学習\n",
    "        #第一段の予測を次の特徴量としてリストにする\n",
    "        merge_feature_list = []\n",
    "        # 初期の学習器で学習を行う\n",
    "        count = 0\n",
    "        for clf_name in self.clfs_index:\n",
    "            #学習器をセット\n",
    "            clf_origin = self.original_clfs[clf_name]\n",
    "            #予測のりすと\n",
    "            preds_temp_list = []\n",
    "            #交差検証実施\n",
    "            for train_index, test_index in index_list:\n",
    "                #この時点で学習器をコピー\n",
    "                clf_copy = deepcopy(clf_origin)\n",
    "                #学習スタート\n",
    "                clf_copy.fit(X[train_index], y[train_index])\n",
    "                #予測のリストを格納\n",
    "                preds_temp_list.extend(\n",
    "                (clf_copy.predict(X[test_index])).tolist())\n",
    "#                 print(preds_temp_list)\n",
    "            \n",
    "                #学習ずみのモデルを格納\n",
    "                self.clf_dict[clf_name].append(clf_copy)\n",
    "            if count == 0:\n",
    "                merge_feature_list = preds_temp_list\n",
    "                merge_feature_list  = np.array(merge_feature_list).reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                preds_temp_list = np.array(preds_temp_list).reshape(-1,1)\n",
    "                merge_feature_list = np.concatenate((merge_feature_list, preds_temp_list), axis=1)\n",
    "\n",
    "        #予測のリストを次の学習の特徴量とする.\n",
    "#             preds_temp_list = np.array(preds_temp_list)\n",
    "#             merge_feature_list.append(preds_temp_list)\n",
    "            count += 1\n",
    "\n",
    "#         print(len(merge_feature_list))\n",
    "        merge_feature_list = np.array(merge_feature_list)\n",
    "        print('第一段階の特徴量のサイズ:{0}'.format(merge_feature_list.shape))\n",
    "        \n",
    "#         #Xも特徴量に追加 <--これは削除しても良い？\n",
    "#         merge_feature_list = np.concatenate((merge_feature_list, X), axis=1)\n",
    "#         print(merge_feature_list.shape)\n",
    "#         print()\n",
    "        \n",
    "    \n",
    "        #第二学習フェーズ\n",
    "        y_merged_second = np.hstack([y[test_index]\n",
    "                              for _, test_index in index_list])\n",
    "        \n",
    "        #リストの初期化\n",
    "        self.clfs_dict_second =  defaultdict(list) \n",
    "        #交差検証(k=5)\n",
    "        K = 5\n",
    "        kf = KFold(n_splits=K, shuffle=True, random_state=1 )\n",
    "        #indexのリストを作成\n",
    "        index_list = list(kf.split(merge_feature_list, y_merged_second))\n",
    "        \n",
    "        merge_feature_list_second = []\n",
    "        # 初期の学習器で学習を行う\n",
    "        count = 0\n",
    "        for clf_name in self.clfs_index_second:\n",
    "            #学習器をセット\n",
    "            clf_origin_second = self.original_clfs_second[clf_name]\n",
    "            #予測のりすと\n",
    "            preds_temp_list_second = []\n",
    "            #交差検証実施\n",
    "            for train_index, test_index in index_list:\n",
    "                #この時点で学習器をコピー\n",
    "                clf_copy_second = deepcopy(clf_origin_second)\n",
    "                #学習スタート\n",
    "                clf_copy_second.fit(merge_feature_list[train_index], y_merged_second[train_index])\n",
    "                #予測のリストを格納\n",
    "                preds_temp_list_second.extend(\n",
    "                (clf_copy_second.predict(merge_feature_list[test_index])).tolist())\n",
    "#                 print(preds_temp_list)\n",
    "            \n",
    "                #学習ずみのモデルを格納\n",
    "                self.clf_dict_second[clf_name].append(clf_copy_second)\n",
    "            if count == 0:\n",
    "                merge_feature_list_second = preds_temp_list_second\n",
    "                merge_feature_list_second  = np.array(merge_feature_list_second).reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                preds_temp_list_second = np.array(preds_temp_list_second).reshape(-1,1)\n",
    "                merge_feature_list_second = np.concatenate((merge_feature_list_second, preds_temp_list_second), axis=1)\n",
    "\n",
    "        #予測のリストを次の学習の特徴量とする.\n",
    "#             preds_temp_list = np.array(preds_temp_list)\n",
    "#             merge_feature_list.append(preds_temp_list)\n",
    "            count += 1\n",
    "\n",
    "#         print(len(merge_feature_list))\n",
    "            merge_feature_list_second = np.array(merge_feature_list_second)\n",
    "#         print(merge_feature_list)\n",
    "\n",
    "        print('第二段階の特徴量のサイズ:{0}'.format(merge_feature_list_second.shape))\n",
    "    \n",
    "        #第三学習フェーズ\n",
    "        X_merged = merge_feature_list_second\n",
    "        \n",
    "        print(X_merged.shape)\n",
    "        \n",
    "#         print(X_merged.shape)\n",
    "        y_merged = np.hstack([y_merged_second[test_index]\n",
    "                              for _, test_index in index_list])\n",
    "        print(y_merged.shape)\n",
    "        \n",
    "        self.m_clf.fit(X_merged, y_merged)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #第一段階\n",
    "        #特徴量のリストを作成\n",
    "        merge_feature_for_test = []\n",
    "        # それぞれの学習器で学習\n",
    "        count = 0\n",
    "        for clf_name in self.clfs_index:\n",
    "        #予測のリストを作成\n",
    "            print(clf_name)\n",
    "            temp_proba_list = []\n",
    "            # 各学習済みモデルを引き出す\n",
    "            for clf in self.clf_dict[clf_name]:\n",
    "                #予測のリストに各学習器の予測を追加\n",
    "                temp_proba_list.append(clf.predict(X))\n",
    "            # 予測を行方向に平均をとる。\n",
    "            merge_feature_for_test.append(\n",
    "                np.mean(temp_proba_list, axis=0))\n",
    "            \n",
    "        print('(予測)第一段階の特徴量のサイズ:{0}'.format(np.array(merge_feature_for_test).T))\n",
    "        \n",
    "\n",
    "            \n",
    "        #第二段階\n",
    "        #特徴量のリストを作成\n",
    "        X_merged_second = np.array(merge_feature_for_test).T\n",
    "#         #Xも特徴量に追加 <--これは削除しても良い？\n",
    "#         X_merged_second = np.concatenate((X_merged_second, X), axis=1)\n",
    "        print(X_merged_second.shape)\n",
    "        merge_feature_for_test_second = []\n",
    "        # それぞれの学習器で学習\n",
    "        count = 0\n",
    "        for clf_name in self.clfs_index_second:\n",
    "            print(clf_name)\n",
    "        #予測のリストを作成\n",
    "            temp_proba_list_second = []\n",
    "            # 各学習済みモデルを引き出す\n",
    "            for clf in self.clf_dict_second[clf_name]:\n",
    "                #予測のリストに各学習器の予測を追加\n",
    "                temp_proba_list_second.append(clf.predict(X_merged_second))\n",
    "            # 予測を行方向に平均をとる。\n",
    "            merge_feature_for_test_second.append(\n",
    "                np.mean(temp_proba_list_second, axis=0))\n",
    "            \n",
    "        print('(予測)第二段階の特徴量のサイズ:{0}'.format(np.array(merge_feature_for_test_second).T))\n",
    "\n",
    "            \n",
    "        # 特徴量リストを\n",
    "        X_merged = np.array(merge_feature_for_test_second).T\n",
    "        print(X_merged.shape)\n",
    "        print(X_merged[:,0])\n",
    "        X_merged_harmonic_mean = 1/((1/X_merged[:,0]  + 1/X_merged[:, 1]  + 1/X_merged[:,2] ) * (1/3))\n",
    "        print(X_merged_harmonic_mean)\n",
    "        print(X_merged_harmonic_mean.shape)\n",
    "        predict_X = np.average(X_merged, axis=1)\n",
    "        print(predict_X)      \n",
    "        print(np.exp(predict_X))\n",
    "\n",
    "\n",
    "\n",
    "        return X_merged_harmonic_mean\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge= linear_model.Ridge(**clf_ridge_cv.best_params_)#0.7\n",
    "xgboost =  xgb.XGBRegressor(**clf_xgb_cv.best_params_)#0.7(**clf_xgb_cv.best_params_)#0.7\n",
    "rf = RandomForestRegressor(**clf_rf_cv.best_params_)\n",
    "lgbm = lgb.LGBMRegressor(lerning_rate=0.09, max_depth=6, n_estimators=100)\n",
    "# svr = SVR(C=gridsearch.best_params_[\"C\"], epsilon=gridsearch.best_params_[\"epsilon\"])\n",
    "est= GradientBoostingRegressor(**clf_cv.best_params_)#0.7\n",
    "Bridge = BayesianRidge( **clf_Bridge_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一段階で用いるモデルを定義\n",
    "first_estimators = list(zip([\"ridge\",\"xgb\",\"Bridge\", \"rf\",\"lgb\", \"est\"],\n",
    "                          [ridge, xgboost,Bridge,  rf, lgbm, est]))\n",
    "#第二段階で用いるモデルを定義\n",
    "second_estimators = list(zip([\"xgb\", \"ridge\" ,\"lgb\"],[ xgboost, ridge, lgbm]))\n",
    "\n",
    "#スタッキングクラスの定義\n",
    "clf_stcl = StackingClassifer(first_estimators, second_estimators , xgb.XGBRegressor(**clf_xgb_cv.best_params_))#xgb.XGBRegressor(**clf_xgb_cv.best_params_)\n",
    "\n",
    " #学習\n",
    "clf_stcl.fit(X_train, y_train)\n",
    "\n",
    "#評価\n",
    "print(rmsle(np.exp(clf_stcl.predict(X_train)), np.exp(y_train)))\n",
    "print(np.exp(y_train))\n",
    "\n",
    "#モデルの保存\n",
    "with open('model_stcl_develop.pickle', mode='wb') as fp:\n",
    "    pickle.dump(clf_stcl, fp, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
